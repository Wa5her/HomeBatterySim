{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API call Functions\n",
    "This section handles all API calls to octopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "# Your Octopus Energy API key\n",
    "\n",
    "\n",
    "# API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "\n",
    "# Function to fetch electricity consumption data for a specific MPAN and meter serial number\n",
    "\n",
    "\n",
    "def get_electricity_consumption(mpan, meter_serial_number, period_from, period_to):\n",
    "\n",
    "    url = f\"https://api.octopus.energy/v1/electricity-meter-points/{mpan}/meters/{\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        meter_serial_number}/consumption?page_size=14000&period_from={period_from}&period_to={period_to}\"\n",
    "\n",
    "    response = requests.get(url, auth=(API_KEY, ''))\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "# Function to fetch tariff prices for a specific product and tariff code\n",
    "\n",
    "\n",
    "def get_tariff_prices_old(tariff_code, period_from, period_to):\n",
    "    product_code = tariff_code[5:-2]\n",
    "\n",
    "    url = f\"https://api.octopus.energy/v1/products/{product_code}/electricity-tariffs/{\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tariff_code}/standard-unit-rates?page_size=14000&period_from={period_from}&period_to={period_to}\"\n",
    "\n",
    "    # print(url)\n",
    "    response = requests.get(url, auth=(API_KEY, ''))\n",
    "\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_tariff_prices(tariff_code, period_from, period_to):\n",
    "    product_code = tariff_code[5:-2]\n",
    "    url = f\"https://api.octopus.energy/v1/products/{product_code}/electricity-tariffs/{\n",
    "        tariff_code}/standard-unit-rates?page_size=14000&period_from={period_from}&period_to={period_to}\"\n",
    "    response = requests.get(url, auth=(API_KEY, ''))\n",
    "    data = response.json()\n",
    "    all_data = data['results']\n",
    "\n",
    "    while data['next'] is not None:\n",
    "        # Get the next page URL\n",
    "        next_page_url = data['next']\n",
    "        response = requests.get(next_page_url, auth=(API_KEY, ''))\n",
    "        data = response.json()\n",
    "        all_data.extend(data['results'])\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "# Main function to gather consumption data and tariff costs\n",
    "\n",
    "\n",
    "def get_electricity_data(account_number, start_date, end_date):\n",
    "\n",
    "    # Get account details to get MPAN and meter information\n",
    "\n",
    "    account_url = f\"https://api.octopus.energy/v1/accounts/{account_number}/\"\n",
    "\n",
    "    account_response = requests.get(account_url, auth=(API_KEY, ''))\n",
    "\n",
    "    account_data = account_response.json()\n",
    "\n",
    "    # Extract MPAN and meter information\n",
    "\n",
    "    mpans = [meter_point['mpan']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "             for meter_point in account_data['properties'][0]['electricity_meter_points']]\n",
    "\n",
    "    meter_serial_numbers = [meter['serial_number']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            for meter in account_data['properties'][0]['electricity_meter_points'][0]['meters']]\n",
    "    tariff_code = account_data['properties'][0]['electricity_meter_points'][0]['agreements'][0]['tariff_code']\n",
    "\n",
    "    # Fetch consumption data for each MPAN and meter\n",
    "\n",
    "    consumption_data = {}\n",
    "    mpan_file = open('mpan_meter.csv', 'w')\n",
    "\n",
    "    for mpan in mpans:\n",
    "\n",
    "        for meter_serial_number in meter_serial_numbers:\n",
    "\n",
    "            mpan_file.write(f'{mpan}-{meter_serial_number}\\n')\n",
    "\n",
    "            consumption_data[(mpan, meter_serial_number)] = get_electricity_consumption(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                mpan, meter_serial_number, start_date, end_date)\n",
    "            with open(f'{mpan}-{meter_serial_number}.json', 'w') as f:\n",
    "                json.dump(consumption_data[(mpan, meter_serial_number)], f)\n",
    "\n",
    "    mpan_file.close()\n",
    "\n",
    "    # Fetch tariff prices for all tariffs associated with the account\n",
    "    tariff_file = open('tariffs.csv', 'w')\n",
    "\n",
    "    tariff_codes = [tariff['tariff_code'] for tariff in account_data['properties']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    [0]['electricity_meter_points'][0]['agreements']]\n",
    "\n",
    "    tariff_prices = {}\n",
    "\n",
    "    for tariff_code in tariff_codes:\n",
    "        tariff_file.write(f'{tariff_code}\\n')\n",
    "\n",
    "        tariff_prices[tariff_code] = get_tariff_prices(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            tariff_code, start_date, end_date)\n",
    "        with open(f'{tariff_code}.json', 'w') as f:\n",
    "            json.dump(tariff_prices[tariff_code], f)\n",
    "\n",
    "    tariff_file.close()\n",
    "\n",
    "    return {\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"consumption_data\": consumption_data,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \"tariff_prices\": tariff_prices\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API calling execution\n",
    "This part is the actual execution to get tarrifs and consumption based on functions above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage example\n",
    "account_number = \"A-3FAEDC7D\"\n",
    "start_date = \"2024-04-19T01:00:00Z\"\n",
    "end_date = \"2025-01-27T00:00:00Z\"\n",
    "\n",
    "# load keys\n",
    "\n",
    "# get API key from file\n",
    "with open('apikey.txt', 'r') as file:\n",
    "    API_KEY = file.read()\n",
    "\n",
    "electricity_data = get_electricity_data(account_number, start_date, end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download energy selling price in Agile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get export cost data\n",
    "import_data = get_tariff_prices(\n",
    "    'E-1R-AGILE-BB-24-10-01-H', start_date, end_date)\n",
    "export_data = get_tariff_prices(\n",
    "    'E-1R-AGILE-OUTGOING-BB-23-02-28-H', start_date, end_date)\n",
    "with open('export.json', 'w') as f:\n",
    "    json.dump(export_data, f)\n",
    "with open('import.json', 'w') as f:\n",
    "    json.dump(import_data, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this section after each time the data gets downloaded to convert Jsons to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted export.json to export.csv\n",
      "Successfully converted 2600002565617-22L4294637.json to 2600002565617-22L4294637.csv\n",
      "Successfully converted E-1R-INTELLI-VAR-22-10-14-H.json to E-1R-INTELLI-VAR-22-10-14-H.csv\n",
      "Successfully converted import.json to import.csv\n",
      "Successfully converted E-1R-VAR-22-11-01-H.json to E-1R-VAR-22-11-01-H.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "def json_to_csv(json_file_path, csv_file_path):\n",
    "    \"\"\"\n",
    "    Converts a JSON file to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        json_file_path: Path to the JSON file.\n",
    "        csv_file_path: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    \n",
    "   \n",
    "    try:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        try:\n",
    "            results = data.get('results', [])  # Extract the 'results' list\n",
    "        except AttributeError:\n",
    "            results = data\n",
    "\n",
    "        if not results:\n",
    "            print(\"No 'results' found in the JSON file.\")\n",
    "            return\n",
    "\n",
    "        # Extract field names from the first dictionary in results\n",
    "        fieldnames = results[0].keys() if results else []\n",
    "\n",
    "        with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()  # Write the header row\n",
    "            writer.writerows(results)  # Write the data rows\n",
    "\n",
    "        print(f\"Successfully converted {json_file_path} to {csv_file_path}\")\n",
    "        \n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: JSON file not found at {json_file_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {json_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "\n",
    "\n",
    "# Get the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "for filename in os.listdir(current_dir):\n",
    "    # Check if the file is a JSON file\n",
    "    if filename.endswith(\".json\"):\n",
    "\n",
    "        # Construct the full file path\n",
    "        json_file_name = filename\n",
    "        # Define the output CSV file name based on the JSON file name\n",
    "        csv_file_name = json_file_name.replace(\".json\", \".csv\")\n",
    "        json_to_csv(json_file_name, csv_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# breakpoint All files downloaded now. \n",
    "## Rest of this code is junk. Develop code to merge 3 CSV files - consumption, Buy rate, sell rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File configuration for lookup\n",
    "consumption_file = '2600002565617-22L4294637.csv'\n",
    "buy_rate_file = 'import.csv'\n",
    "sell_rate_file = 'export.csv'\n",
    "#test = pd.read_csv(consumption_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy rate duplicates: 0\n",
      "Sell rate duplicates: 0\n",
      "Consumption interval_start type: object\n",
      "Buy rate valid_from type: datetime64[ns, UTC]\n",
      "Sell rate valid_from type: datetime64[ns, UTC]\n",
      "Error during merge_asof: Incompatible merge dtype, datetime64[ns, UTC] and dtype('O'), both sides must have numeric dtype\n",
      "Falling back to less efficient method...\n",
      "       consumption             interval_start               interval_end  \\\n",
      "0            0.065  2024-04-19 02:00:00+01:00  2024-04-19 02:30:00+01:00   \n",
      "1            0.072  2024-04-19 02:30:00+01:00  2024-04-19 03:00:00+01:00   \n",
      "2            0.097  2024-04-19 03:00:00+01:00  2024-04-19 03:30:00+01:00   \n",
      "3            0.074  2024-04-19 03:30:00+01:00  2024-04-19 04:00:00+01:00   \n",
      "4            0.066  2024-04-19 04:00:00+01:00  2024-04-19 04:30:00+01:00   \n",
      "...            ...                        ...                        ...   \n",
      "13578        0.426  2025-01-26 22:00:00+00:00  2025-01-26 22:30:00+00:00   \n",
      "13579        0.838  2025-01-26 22:30:00+00:00  2025-01-26 23:00:00+00:00   \n",
      "13580        0.445  2025-01-26 23:00:00+00:00  2025-01-26 23:30:00+00:00   \n",
      "13581        0.296  2025-01-26 23:30:00+00:00  2025-01-27 00:00:00+00:00   \n",
      "13582        0.287  2025-01-27 00:00:00+00:00  2025-01-27 00:30:00+00:00   \n",
      "\n",
      "       buy_rate_inc_vat  sell_rate_inc_vat  \n",
      "0                   NaN               4.03  \n",
      "1                   NaN               3.34  \n",
      "2                   NaN               3.56  \n",
      "3                   NaN               3.55  \n",
      "4                   NaN               4.13  \n",
      "...                 ...                ...  \n",
      "13578           11.2455               5.72  \n",
      "13579            2.6460               2.06  \n",
      "13580           10.5420               5.42  \n",
      "13581            7.2345               4.01  \n",
      "13582               NaN                NaN  \n",
      "\n",
      "[13583 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_rates_to_consumption_optimized(consumption_file, buy_rate_file, sell_rate_file):\n",
    "    \"\"\"\n",
    "    Adds buy and sell rate columns to consumption data, with debugging and a fallback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read data (optimized as before)\n",
    "        consumption_df = pd.read_csv(\n",
    "            consumption_file,\n",
    "            parse_dates=[\"interval_start\", \"interval_end\"],\n",
    "            dtype={\"consumption\": \"float32\"},\n",
    "        )\n",
    "        buy_rate_df = pd.read_csv(\n",
    "            buy_rate_file,\n",
    "            parse_dates=[\"valid_from\", \"valid_to\"],\n",
    "            dtype={\"value_inc_vat\": \"float32\"},\n",
    "        )\n",
    "        sell_rate_df = pd.read_csv(\n",
    "            sell_rate_file,\n",
    "            parse_dates=[\"valid_from\", \"valid_to\"],\n",
    "            dtype={\"value_inc_vat\": \"float32\"},\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: One or more input files not found.\")\n",
    "        return None\n",
    "\n",
    "    # Prepare dataframes for merge_asof\n",
    "    consumption_df = consumption_df.sort_values(\"interval_start\")\n",
    "    buy_rate_df = buy_rate_df.sort_values(\"valid_from\")\n",
    "    sell_rate_df = sell_rate_df.sort_values(\"valid_from\")\n",
    "\n",
    "    # Debugging: Check for duplicates again (just to be absolutely sure)\n",
    "    print(\"Buy rate duplicates:\", buy_rate_df[\"valid_from\"].duplicated().sum())\n",
    "    print(\"Sell rate duplicates:\",\n",
    "          sell_rate_df[\"valid_from\"].duplicated().sum())\n",
    "\n",
    "    # Debugging: Re-confirm data types\n",
    "    print(\"Consumption interval_start type:\",\n",
    "          consumption_df[\"interval_start\"].dtype)\n",
    "    print(\"Buy rate valid_from type:\", buy_rate_df[\"valid_from\"].dtype)\n",
    "    print(\"Sell rate valid_from type:\", sell_rate_df[\"valid_from\"].dtype)\n",
    "\n",
    "    try:\n",
    "        # Perform merge_asof (with error handling)\n",
    "        merged_df = pd.merge_asof(\n",
    "            consumption_df,\n",
    "            buy_rate_df[[\"valid_from\", \"value_inc_vat\"]].rename(\n",
    "                columns={\"value_inc_vat\": \"buy_rate_inc_vat\"}\n",
    "            ),\n",
    "            left_on=\"interval_start\",\n",
    "            right_on=\"valid_from\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "\n",
    "        merged_df = pd.merge_asof(\n",
    "            merged_df,\n",
    "            sell_rate_df[[\"valid_from\", \"value_inc_vat\"]].rename(\n",
    "                columns={\"value_inc_vat\": \"sell_rate_inc_vat\"}\n",
    "            ),\n",
    "            left_on=\"interval_start\",\n",
    "            right_on=\"valid_from\",\n",
    "            direction=\"backward\",\n",
    "        )\n",
    "\n",
    "        merged_df = merged_df.drop(\n",
    "            columns=[\"valid_from_x\", \"valid_from_y\"], errors=\"ignore\"\n",
    "        )\n",
    "        return merged_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during merge_asof: {e}\")\n",
    "        print(\"Falling back to less efficient method...\")\n",
    "\n",
    "        # Fallback: Iterative method (less efficient but should always work)\n",
    "        # (Code from your original, corrected iterative approach)\n",
    "        results = []\n",
    "        for index, row in consumption_df.iterrows():\n",
    "            interval_start = row[\"interval_start\"]\n",
    "            interval_end = row[\"interval_end\"]\n",
    "\n",
    "            buy_rate = buy_rate_df.loc[\n",
    "                (buy_rate_df[\"valid_from\"] < interval_end)\n",
    "                & (buy_rate_df[\"valid_to\"] > interval_start),\n",
    "                \"value_inc_vat\",\n",
    "            ]\n",
    "\n",
    "            sell_rate = sell_rate_df.loc[\n",
    "                (sell_rate_df[\"valid_from\"] < interval_end)\n",
    "                & (sell_rate_df[\"valid_to\"] > interval_start),\n",
    "                \"value_inc_vat\",\n",
    "            ]\n",
    "\n",
    "            buy_rate_value = buy_rate.iloc[0] if not buy_rate.empty else None\n",
    "            sell_rate_value = sell_rate.iloc[0] if not sell_rate.empty else None\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"consumption\": row[\"consumption\"],\n",
    "                    \"interval_start\": interval_start,\n",
    "                    \"interval_end\": interval_end,\n",
    "                    \"buy_rate_inc_vat\": buy_rate_value,\n",
    "                    \"sell_rate_inc_vat\": sell_rate_value,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        result_df = pd.DataFrame(results)\n",
    "        return result_df\n",
    "\n",
    "\n",
    "result_df = add_rates_to_consumption_optimized(\n",
    "    consumption_file, buy_rate_file, sell_rate_file\n",
    ")\n",
    "\n",
    "if result_df is not None:\n",
    "    print(result_df)\n",
    "    result_df.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
